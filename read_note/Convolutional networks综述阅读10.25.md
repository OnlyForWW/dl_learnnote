# Convolutional networks

> 卷积神经网络综述

本文试图解释的三个问题：

> 1、怎样减少对数据驱动的依赖
>
> 2、卷积学习的本质
>
> 3、架构设计的优化问题

## 第一章 促使读者回顾对卷积神经网络的理解

## 第二章 multilayers networks 多层网络

注意，本文回顾问题针对图像处理（visual information processing视觉信息处理）

### Multilayer architectures 多层体系结构

经过发展，引入非线形向形成深度学习。（非标准）

表示学习用在深多层架构中称为深度学习

大多数多层架构堆叠具有交替线性和非线性功能的简单构建块模块，在下文中，这样的网络称之为神经网络network

### natural network 神经网络

神经网络的典型架构，x输入y输出，中间多个堆栈隐藏层h，f函数包含多个饱和非线形函数，如sigmoid函数，详见公式。
上述提到的神经网络发展收到XOR问题影响，发展中止过，知道bp反向传播算法的提出。但这也意味着需要更多的计算资源

使深度神经网络得到发展的另一原因是使用受限玻尔兹曼机器 (RBM) 进行分层无监督预训练得到了良好的效果。受限玻尔兹曼机器算法分析见文章，但是粗略判断就是梯度下降。通过与bp算法结合，实现良好效果
（受限玻尔兹曼机器RBM需要进一步了解）

深度神经网络通过RBM的堆积首次成功实现了一种降维方法，称之为自动编码器auto-encoder，后期又提出了许多种不同的自动编码器，稀疏自动编码器 (SAE) [8]、去噪自动编码器 (DAE) [141、142] 和收缩自动编码器 (CAE) 

### recurrent natural network 递归神经网络（循环神经网络） RNN 

为了处理顺序输入的序列数据，将序列的输入顺序考虑进影响范围，提出了rnn，最简单的表示是ht = σ(wixi+wihi-1)，即一次隐藏层的输出不仅与当前输入有关，还与上一时刻的输入相关。

但是rnn存在的梯度消失问题（与dnn的梯度消失不同，指距离此刻较远信号影响的权重消失），影响其对长期依赖关系建模的能力。
又引出了*短期记忆网络*（long short-term memory，LSTM）

详细解释了LSTM单元的具体结构，暂且不看，李沐有教程。

### Convolutional networks 卷积网络

特别适合于图像处理，根据平移不变性和局部性，可以从全连接层推到来。通过局部操作实现分层特征识别，避免像素的单元素一对一连接。此外，卷积网络通过特征共享减少了架构需要依赖的参数。最后，通过池化层的操作，实现降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。随着网络深度的增加，感受野(receptive ﬁeld)增大。

### Generative adversarial networks 生成对抗网络 GAN

简单介绍GAN结构，由两组对抗的神经网络构成，一组生成，一组判断(鉴别)。期望达到的最终结果为生成网络生成的数据能在鉴别网络中通过，达到以假乱真的效果。具体细节日后学习，GAN网络应用领域包括图像文本合成、图像超分辨率修复、图像修复、纹理合成等。



### Multilayer network training 多层网络训练

多层网络的训练基于完全监督的梯度下降算法，这种算法的成功实现又依赖于bp算法实现计算。实际上SGD随机梯度下降是最长使用的。而在SGD的使用中，为了方便学习率的选择，又有一些方法来实现改进。比如具有动量的SGD等。同时，为了克服监督学习需要大量标记数据等缺点，在一些浅层卷积网络中提出了基于预测稀疏分解（PSD）方法，这是一种无监督学习方法。psd解释说明暂略。

### 在迁移学习中的总结

根据一些论文例子的提出，上述多层网络在迁移学习中表现良好，这种良好表现可以用特征提取来解释，底层特征往往在不同任务中通用（真的吗？不好说，希望以后能有机会细看）。后文又点了三个多层网络用于迁移学习中的值得注意的特点。

## 空间卷积网络Spatial convolutional networks

### Key architectures in the recent evolution of ConvNets 18年左右卷积网络关键架构

#### Alex net 

> Alexnet的提出和成功引发了学界对卷积网络的新一轮热潮，该综述提到alexnet相比与以往网络的三个关键架构更新：
> 1、使用relu函数，减少梯度消失的发生，增快训练速度
> 2、引入dropout暂退法，对抗过拟合（常见的正则化方法）
> 3、数据预处理，采用数据增强方法

#### VGG 

VGG网络使用深层但是较小的卷积核（kernal）

#### GoogLenet



